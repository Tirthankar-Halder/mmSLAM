{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment is Ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np                        # Fundamental package for scientific computing\n",
    "import matplotlib.pyplot as plt           # 2D plotting library producing publication quality figures\n",
    "from IPython.display import clear_output  # Clear the screen\n",
    "import pyrealsense2 as rs                 # Intel RealSense cross-platform open-source API\n",
    "print(\"Environment is Ready\")\n",
    "from plyfile import PlyData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single slide dpeth visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = rs.pipeline()                      # Create a pipeline\n",
    "cfg = rs.config()                         # Create a default configuration\n",
    "print(\"Pipeline is created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Searching Devices..\")\n",
    "selected_devices = []                     # Store connected device(s)\n",
    "\n",
    "for d in rs.context().devices:\n",
    "    selected_devices.append(d)\n",
    "    print(d.get_info(rs.camera_info.name))\n",
    "if not selected_devices:\n",
    "    print(\"No RealSense device is connected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_sensor = depth_sensor = None\n",
    "\n",
    "for device in selected_devices:                         \n",
    "    print(\"Required sensors for device:\", device.get_info(rs.camera_info.name))\n",
    "    for s in device.sensors:                              # Show available sensors in each device\n",
    "        if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "            print(\" - RGB sensor found\")\n",
    "            rgb_sensor = s                                # Set RGB sensor\n",
    "        if s.get_info(rs.camera_info.name) == 'Stereo Module':\n",
    "            depth_sensor = s                              # Set Depth sensor\n",
    "            print(\" - Depth sensor found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorizer = rs.colorizer()                                # Mapping depth data into RGB color space\n",
    "profile = pipe.start(cfg)                                 # Configure and start the pipeline\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,4)) # Show 1 row with 2 columns for Depth and RGB frames\n",
    "title = [\"Depth Image\", \"RGB Image\"]                      # Title for each frame\n",
    "\n",
    "for _ in range(10):                                       # Skip first frames to give syncer and auto-exposure time to adjust\n",
    "    frameset = pipe.wait_for_frames()\n",
    "    \n",
    "for _ in range(5):                                        # Increase to display more frames\n",
    "    frameset = pipe.wait_for_frames()                     # Read frames from the file, packaged as a frameset\n",
    "    depth_frame = frameset.get_depth_frame()              # Get depth frame\n",
    "    color_frame = frameset.get_color_frame()              # Get RGB frame\n",
    "\n",
    "    colorized_streams = []                                # This is what we'll actually display\n",
    "    if depth_frame:\n",
    "        colorized_streams.append(np.asanyarray(colorizer.colorize(depth_frame).get_data()))\n",
    "    if color_frame:\n",
    "        colorized_streams.append(np.asanyarray(color_frame.get_data()))\n",
    "    \n",
    "    for i, ax in enumerate(axs.flatten()):                # Iterate over all (Depth and RGB) colorized frames\n",
    "        if i >= len(colorized_streams): continue          # When getting less frames than expected\n",
    "        plt.sca(ax)                                       # Set the current Axes and Figure\n",
    "        plt.imshow(colorized_streams[i])                  # colorized frame to display\n",
    "        plt.savefig(\"./depth435/output.jpg\")\n",
    "        plt.title(title[i])                               # Add title for each subplot\n",
    "    clear_output(wait=True)                               # Clear any previous frames from the display\n",
    "    plt.tight_layout()                                    # Adjusts display size to fit frames\n",
    "    plt.pause(1)                                          # Make the playback slower so it's noticeable\n",
    "    \n",
    "pipe.stop()                                               # Stop the pipeline\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture pcd and save .ply format or raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv writer\n",
    "import csv\n",
    "from datetime import datetime\n",
    "header = [\n",
    "    \"datetime\",\n",
    "    \"frame_number\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\"\n",
    "    \n",
    "]\n",
    "filename = \"data.csv\"  \n",
    "with open(filename, \"w\") as f:\n",
    "    csv.DictWriter(f, fieldnames=header).writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1242583/904255508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Create point cloud object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# print(points.get_vertices())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Enable depth and color streams\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "# Create an align object\n",
    "# rs.align allows us to perform alignment of depth frames to others frames\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        \n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "        \n",
    "        # Get aligned frames\n",
    "        depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Create point cloud object\n",
    "        pc = rs.pointcloud()\n",
    "        points = pc.calculate(depth_frame)\n",
    "        pc.map_to(color_frame)\n",
    "        # print(points.get_vertices())\n",
    "\n",
    "        dict_dumper = {'datetime': datetime.now()}\n",
    "        data = {\n",
    "            \"datetime\" : datetime.now(),\n",
    "            \"frame_number\": points.get_frame_number(),\n",
    "            \"x\":np.asanyarray(points.get_vertices())['f0'],\n",
    "            \"y\":np.asanyarray(points.get_vertices())['f1'],\n",
    "            \"z\":np.asanyarray(points.get_vertices())['f2']\n",
    "        }\n",
    "        \n",
    "        dict_dumper.update(data)\n",
    "        path = \"./data.csv\"\n",
    "        with open(path, \"a\") as f:\n",
    "            writer = csv.DictWriter(f, header)\n",
    "            writer.writerow(dict_dumper)\n",
    "        # print(\"Frame #:\",points.get_frame_number())\n",
    "        # print(\"Frame size:\", points.get_data_size())\n",
    "        # print(\"Frame profile:\", points.profile)\n",
    "        # # depth = np.asanyarray(points.get_data())\n",
    "        # depth = np.asanyarray(points.as_points().get_data()) \n",
    " \n",
    "        # print(\"Depth shape\", depth.shape)\n",
    "        # print(\"Depth\", depth)  \n",
    "\n",
    "\n",
    "        # Save point cloud data as a .ply file\n",
    "        # points.export_to_ply(\"point_cloud.ply\", color_frame)\n",
    "        # print(\"Saved point cloud to 'point_cloud.ply'\")\n",
    "\n",
    "        # Display the color image\n",
    "        cv2.imshow('RealSense', color_image)\n",
    "\n",
    "        # Exit on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()\n",
    "\n",
    "    # Close OpenCV windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<pyrealsense2.frame NULL>, dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asanyarray(points.as_pose_frame())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize captured pcd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "PointCloud with 266589 points.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Load the .ply file\n",
    "ply_file = \"point_cloud.ply\"\n",
    "pcd = o3d.io.read_point_cloud(ply_file)\n",
    "\n",
    "# Print basic information about the point cloud\n",
    "print(pcd)\n",
    "\n",
    "# Visualize the point cloud\n",
    "o3d.visualization.draw_geometries([pcd], window_name=\"Point Cloud Viewer\", width=800, height=600, left=50, top=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open captured pcd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plydata = PlyData.read(ply_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points:\n",
      "[[-2.140079    1.55787    -4.03      ]\n",
      " [-2.133443    1.55787    -4.03      ]\n",
      " [-2.126807    1.55787    -4.03      ]\n",
      " ...\n",
      " [ 0.23667763 -0.18376403 -0.45700002]\n",
      " [ 0.23898879 -0.18497035 -0.46      ]\n",
      " [ 0.23974624 -0.18497035 -0.46      ]]\n",
      "Colors:\n",
      "[[ 77 107 110]\n",
      " [ 77 107 110]\n",
      " [ 77 107 110]\n",
      " ...\n",
      " [ 77 107 110]\n",
      " [ 77 107 110]\n",
      " [ 77 107 110]]\n"
     ]
    }
   ],
   "source": [
    "# Extract vertex data\n",
    "vertex_data = plydata['vertex'].data\n",
    "\n",
    "# Convert to numpy array\n",
    "points = np.vstack([vertex_data['x'], vertex_data['y'], vertex_data['z']]).T\n",
    "\n",
    "# Extract colors if available\n",
    "colors = None\n",
    "if 'red' in vertex_data.dtype.names:\n",
    "    colors = np.vstack([vertex_data['red'], vertex_data['green'], vertex_data['blue']]).T\n",
    "\n",
    "# Extract normals if available\n",
    "normals = None\n",
    "if 'nx' in vertex_data.dtype.names:\n",
    "    normals = np.vstack([vertex_data['nx'], vertex_data['ny'], vertex_data['nz']]).T\n",
    "\n",
    "# Print the extracted data\n",
    "print(\"Points:\")\n",
    "print(points)\n",
    "if colors is not None:\n",
    "    print(\"Colors:\")\n",
    "    print(colors)\n",
    "if normals is not None:\n",
    "    print(\"Normals:\")\n",
    "    print(normals)\n",
    "\n",
    "# Save extracted data to files if needed\n",
    "np.savetxt(\"points.csv\", points, delimiter=\",\")\n",
    "if colors is not None:\n",
    "    np.savetxt(\"colors.csv\", colors, delimiter=\",\")\n",
    "if normals is not None:\n",
    "    np.savetxt(\"normals.csv\", normals, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZE EXISTING CAPTURE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://librealsense.intel.com/rs-tests/TestData/stairs.bag\n",
    "print(\"Data is Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup:\n",
    "pipe = rs.pipeline()\n",
    "cfg = rs.config()\n",
    "cfg.enable_device_from_file(\"stairs.bag\")\n",
    "profile = pipe.start(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##capture depth frame\n",
    "\n",
    "# Skip 5 first frames to give the Auto-Exposure time to adjust\n",
    "for x in range(5):\n",
    "  pipe.wait_for_frames()\n",
    "  \n",
    "# Store next frameset for later processing:\n",
    "frameset = pipe.wait_for_frames()\n",
    "depth_frame = frameset.get_depth_frame()\n",
    "\n",
    "# Cleanup:\n",
    "pipe.stop()\n",
    "print(\"Frames Captured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorizer = rs.colorizer()\n",
    "colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.imshow(colorized_depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
